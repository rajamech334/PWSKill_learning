{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fbe02e3-42e2-4957-a7e4-82ae735c5e89",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some \n",
    "algorithms that are not affected by missing values.\n",
    "\n",
    "Ans)\n",
    "\n",
    "\n",
    "Missing values in a dataset occur when no data value is stored for a variable in an observation. This can happen for various reasons, such as errors in data collection, data entry issues, or non-responses in surveys.\n",
    "\n",
    "Importance of Handling Missing Values:\n",
    "\n",
    "1. Bias and Accuracy: Missing data can introduce bias, leading to inaccurate conclusions or predictions.\n",
    "2. Statistical Power: Missing values reduce the amount of available data, potentially decreasing the statistical power of analyses.\n",
    "3. Algorithm Requirements: Many machine learning algorithms cannot handle missing values directly and may fail or provide erroneous results if missing values are present.\n",
    "4. Data Integrity: Handling missing values ensures the integrity and completeness of the dataset, which is crucial for reliable data analysis and model building\n",
    "\n",
    "Algorithms that are not affected by missing values:\n",
    "\n",
    "1. Tree-Based Methods:\n",
    "\n",
    "    1. Decision Trees: CART (Classification and Regression Trees), C4.5, etc.\n",
    "    2. Random Forest: An ensemble method using multiple decision trees.\n",
    "    3. Gradient Boosting Machines (GBM): Including variants like LightGBM.\n",
    "    4. XGBoost: Extreme Gradient Boosting.\n",
    "    \n",
    "2. k-Nearest Neighbors (k-NN):\n",
    "\n",
    "    1. Some implementations of k-NN can handle missing values by ignoring them during distance calculations.\n",
    "    \n",
    "3. Naive Bayes:\n",
    "\n",
    "    1. Can handle missing values by considering only the available data for each attribute when calculating probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4dd95be-a852-4b22-8756-b59e61c6305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listwise Deletion\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n",
      "\n",
      "***************************\n",
      "\n",
      "Listwise Deletion\n",
      "Mean A: 2.3333333333333335, Mean B: 6.666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Q2: List down techniques used to handle missing data.  Give an example of each with python code.\n",
    "#Ans)\n",
    "\"\"\"\n",
    "1. Deletion:\n",
    "    1. Listwise Deletion: Remove rows with any missing values.\n",
    "    2. Pairwise Deletion: Remove rows only for specific analyses where values are missing.\n",
    "\"\"\"\n",
    "\n",
    "#Listwise Deletion\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Listwise Deletion\n",
    "df_listwise = df.dropna()\n",
    "print(\"Listwise Deletion\")\n",
    "print(df_listwise)\n",
    "\n",
    "print(\"\\n***************************\\n\")\n",
    "# Pairwise deletion\n",
    "print(\"Listwise Deletion\")\n",
    "mean_A = df['A'].mean()\n",
    "mean_B = df['B'].mean()\n",
    "print(f\"Mean A: {mean_A}, Mean B: {mean_B}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f61a57-c1ed-40df-b9be-1deb42a0f386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Imputation\n",
      "\n",
      "          A         B\n",
      "0  1.000000  5.000000\n",
      "1  2.000000  6.666667\n",
      "2  2.333333  7.000000\n",
      "3  4.000000  8.000000\n",
      "\n",
      "Median Imputation\n",
      "\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  7.0\n",
      "2  2.0  7.0\n",
      "3  4.0  8.0\n",
      "\n",
      "Mode Imputation\n",
      "\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  5.0\n",
      "2  1.0  7.0\n",
      "3  4.0  8.0\n",
      "\n",
      "Backward Fill\n",
      "\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  7.0\n",
      "2  4.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. Imputation:\n",
    "    1. Mean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the column.\n",
    "    \n",
    "    2. Forward Fill/Backward Fill: Use the previous or next value to fill in the missing value.\n",
    "    \n",
    "    3. Predictive Imputation: Use machine learning algorithms to predict and fill missing values based on other available data.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"\\nMean Imputation\\n\")\n",
    "# Mean Imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_mean_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_mean_imputed)\n",
    "\n",
    "print(\"\\nMedian Imputation\\n\")\n",
    "# Median Imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_median_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_median_imputed)\n",
    "\n",
    "print(\"\\nMode Imputation\\n\")\n",
    "# Mode Imputation\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_mode_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_mode_imputed)\n",
    "\n",
    "print(\"\\nBackward Fill\\n\")\n",
    "# Backward Fill\n",
    "df_bfill = df.fillna(method='bfill')\n",
    "print(df_bfill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a01f31b-6370-475a-96a4-102d7dc3fa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.5\n",
      "2  2.5  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3. Predictive Imputation-K-Nearest Neighbors Imputation\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Sample data\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# KNN Imputation\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_knn_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_knn_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877593d1-b62b-4f6a-89de-45c74563448e",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "Ans)\n",
    "\n",
    "Imbalanced data refers to datasets where the classes are not represented equally. For example, in a binary classification problem, if 95% of the instances belong to one class and only 5% to the other, the data is highly imbalanced.\n",
    "\n",
    "Consequences of Not Handling Imbalanced Data:\n",
    "\n",
    "1. Bias Towards Majority Class: Machine learning models may become biased towards the majority class, predicting it more frequently and often ignoring the minority class.\n",
    "\n",
    "2. Poor Model Performance: Key performance metrics such as accuracy might appear high, but the model's performance on the minority class (often the class of interest) will be poor.\n",
    "\n",
    "3. Skewed Metrics: Traditional metrics like accuracy become misleading. For example, in a dataset with 95% of one class, a model that always predicts the majority class will have 95% accuracy but 0% recall for the minority class.\n",
    "\n",
    "4. Misleading Insights: Business decisions based on the model's outputs might be flawed if the model fails to accurately predict the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a0ef3-f2aa-41cf-99f4-2dcd8d3bdc7f",
   "metadata": {},
   "source": [
    "4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and downsampling are required.\n",
    "\n",
    "Ans)\n",
    "\n",
    "Up-sampling:\n",
    "Up-sampling involves increasing the number of instances in the minority class to match the number of instances in the majority class. This can help balance the dataset and make the model more sensitive to the minority class.\n",
    "\n",
    "Down-sampling:\n",
    "Down-sampling involves reducing the number of instances in the majority class to match the number of instances in the minority class. This can help balance the dataset but may lead to a loss of valuable information from the majority class.\n",
    "\n",
    "When to Use Up-sampling and Down-sampling\n",
    "\n",
    "1. Up-sampling:\n",
    "\n",
    "    1. Use when the minority class is significantly underrepresented, and you want to increase the sensitivity of the model towards this class.\n",
    "    2. Beneficial when you have a small dataset, and removing majority class instances is not desirable.\n",
    "\n",
    "2. Down-sampling:\n",
    "\n",
    "    1. Use when the dataset is large, and removing some instances from the majority class will not significantly affect the model's performance.\n",
    "    2. Useful when up-sampling would result in an excessively large dataset, making it computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45d9ec7-7cd3-4daf-8c37-d1a5c97611d9",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE?\n",
    "\n",
    "Ans)\n",
    "\n",
    "Data augmentation is a technique used to increase the diversity of a dataset without actually collecting new data. In the context of machine learning, it often involves creating new training examples by applying various transformations to the existing data\n",
    "\n",
    "SMOTE:\n",
    "\n",
    "SMOTE is a popular data augmentation technique specifically designed to address class imbalance in datasets. It generates synthetic samples for the minority class by interpolating between existing minority class examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0008b2-b26b-4738-ba75-9aec2f63e2df",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "Ans)\n",
    "\n",
    "Outliers are data points that deviate significantly from the majority of the data. They can be much higher or lower than the other values in the dataset and do not fit the general pattern of the data.\n",
    "\n",
    "Why It Is Essential to Handle Outliers:\n",
    "\n",
    "1. Impact on Statistical Measures: Outliers can significantly affect the mean and standard deviation of a dataset, leading to misleading statistical summaries.\n",
    "\n",
    "2. Impact on Machine Learning Models: Outliers can distort the training process of machine learning models, leading to poor performance and less accurate predictions.\n",
    "\n",
    "3. Data Integrity: Outliers can indicate errors in data collection, entry, or processing, compromising the integrity of the dataset\n",
    "\n",
    "4. Robustness and Generalization:  Handling outliers can make models more robust and better generalize to unseen data, as they are less influenced by extreme values that are not representative of the overall data distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd75fba-0675-401a-9790-b361aed12823",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of \n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "Ans)\n",
    "\n",
    "When analyzing customer data, missing values can pose significant challenges and potentially skew the results of the analysis. It is crucial to handle these missing values appropriately to maintain the integrity and accuracy of the analysis. Following are the a few techniques/Methods to handle it.\n",
    "\n",
    "1. Deletion Methods:\n",
    "\n",
    "    a. Listwise deletion: It involves removing any data rows that contain missing values. This method is straightforward and ensures that analyses are performed on complete data sets\n",
    "    \n",
    "    b. Pairwise Deletion : It only excludes missing data for specific analyses. For example, when calculating correlations, only the pairs of values that are complete are considered\n",
    "\n",
    "2. Imputation Methods:\n",
    "\n",
    "    a. Mean/Median/Mode Imputation: This technique replaces missing values with the mean, median, or mode of the respective column. It is simple to implement but may distort the data distribution by reducing variance and potentially creating biases\n",
    "    \n",
    "    b. Forward Fill/Backward Fill: Forward fill replaces missing values with the last observed value, while backward fill uses the next observed value. These methods are useful for time-series data where continuity is essential, but they can propagate errors if the missing data is not randomly distributed\n",
    "    \n",
    "    c. Predictive Imputation : Predictive imputation uses machine learning models to predict and fill missing values based on other variables in the dataset. Techniques such as regression, decision trees, or more complex algorithms can be used. This method can provide more accurate imputations but requires careful model selection and validation.\n",
    "    \n",
    "    \n",
    "3. Advanced Imputation Methods:\n",
    "\n",
    "    a. K-Nearest Neighbors (KNN) Imputation: KNN imputation replaces missing values with the average value of the k-nearest neighbors. This method considers the similarity between data points, providing more contextually relevant imputations\n",
    "    \n",
    "    b. Multiple Imputation: Multiple imputation involves generating multiple datasets with different imputed values, analyzing each dataset separately, and then combining the results.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6fa7e3-b7e6-4caa-b46c-e85f2ef4e248",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are \n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern \n",
    "to the missing data?\n",
    "\n",
    "Ans)\n",
    "\n",
    "Ther are a few strategies to handle the missing data at random.\n",
    "\n",
    "1. Visual Inspection:\n",
    "\n",
    "    a. Missing Data Matrix : A missing data matrix visualizes the presence and absence of data points. By plotting the matrix, you can see if there are clusters or patterns in the missing data\n",
    "    \n",
    "    b. Pattern Plots: Visualizing missing data patterns for pairs or groups of variables can help identify relationships.\n",
    "    \n",
    "2. Statistical Tests:\n",
    "    \n",
    "    a. Little’s MCAR Test: Little’s Missing Completely at Random (MCAR) test is a statistical test used to determine if the missing data is MCAR. If the test is not significant, it suggests that the data is MCAR, meaning the missingness is unrelated to any values, observed or unobserved.\n",
    "    \n",
    "    b. Chi-Square Test for Independence: A chi-square test can be used to test the independence between the missingness of different variables. If the test shows a significant association, it indicates that the missing data in one variable is related to the missing data in another, suggesting that the data is not missing completely at random.\n",
    "    \n",
    "3. Correlation Analysis:\n",
    "    \n",
    "    a. Correlation of Missingness: Analyzing the correlations between missingness indicators for different variables can help identify patterns. High correlations between the indicators suggest that the missingness in one variable may be related to the missingness in another, indicating that the data is not missing at random\n",
    "    \n",
    "4. Pattern Analysis:\n",
    "\n",
    "    a. Missing Data Patterns: Examining the specific patterns of missing data across the dataset can help identify systematic missingness.\n",
    "    \n",
    "5. Predictive Modeling:\n",
    "\n",
    "    a. Modeling Missing Data Patterns: Using predictive models to understand missing data can provide insights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c254f-b125-417d-961a-b4c3f159af62",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the \n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you \n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "Ans)\n",
    "\n",
    "Typically imbalanced datasets pose challenges for machine learning models because the models may become biased toward the majority class, leading to poor performance in identifying the minority class.\n",
    "Following are some strategies to evaluate the performance of machine learning models\n",
    "\n",
    "1. Confusion Matrix and Derived Metrics : The confusion matrix provides a comprehensive view of the performance of the classification model by showing the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN)\n",
    "\n",
    "2. ROC Curve and AUC: \n",
    "    a. ROC Curve (Receiver Operating Characteristic Curve): A graphical representation of the model's performance across different classification thresholds. The ROC curve plots the true positive rate (sensitivity) against the false positive rate (1 - specificity)\n",
    "    \n",
    "    b. AUC (Area Under the Curve): The area under the ROC curve. AUC provides a single metric that summarizes the model's ability to discriminate between the positive and negative classes. A higher AUC indicates better model performance.\n",
    "    \n",
    "3. Precision-Recall Curve and AUC:\n",
    "    \n",
    "    a. Precision-Recall Curve: A plot of precision versus recall for different classification thresholds. This curve is particularly useful for imbalanced datasets as it focuses on the performance of the minority class.\n",
    "\n",
    "    b. AUC-PR (Area Under the Precision-Recall Curve): The area under the precision-recall curve. This metric provides a summary of the model's performance in terms of precision and recall.\n",
    "    \n",
    "4. Balanced Accuracy: Balanced accuracy is the average of sensitivity and specificity. It accounts for imbalances in the dataset by considering both true positive and true negative rates.\n",
    "\n",
    "5. Resampling Techniques:\n",
    "\n",
    "    a. Oversampling: Increase the number of instances in the minority class by duplicating or generating synthetic samples (e.g., using SMOTE - Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "    b. Undersampling: Reduce the number of instances in the majority class by randomly removing samples.\n",
    "\n",
    "    c. Combination of Oversampling and Undersampling: A balanced approach that uses both techniques to create a more balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e5669-16a8-4bf1-818a-333705c7f33c",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is \n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to \n",
    "balance the dataset and down-sample the majority class?\n",
    "\n",
    "Ans)\n",
    "\n",
    "To handle give senario we can employ several methods to balance the dataset, particularly focusing on down-sampling the majority class. Following are some theoretical approaches.\n",
    "\n",
    "1. Random Down-Sampling: \n",
    "Random down-sampling involves reducing the number of instances in the majority class by randomly removing samples until the classes are balanced. This method can help mitigate the imbalance but may result in the loss of valuable information from the majority class.\n",
    "\n",
    "2. Cluster-Based Down-Sampling:\n",
    "Cluster-based down-sampling involves clustering the majority class into different groups and then selecting representative samples from each cluster. This method aims to retain the diversity within the majority class while reducing its size.\n",
    "\n",
    "3. Stratified Down-Sampling:\n",
    "Stratified down-sampling ensures that the samples selected from the majority class maintain the same distribution of important features as the original majority class. This helps in preserving the structure and relationships within the majority class.\n",
    "\n",
    "4. Under-Sampling with Tomek Links\n",
    "Tomek Links are pairs of instances where each instance in the pair belongs to a different class, and they are the nearest neighbors to each other. By removing the majority class instances that form Tomek Links, the dataset can be balanced while potentially improving the decision boundary between classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d3e52a-4a1e-4fda-81fc-fe994ad4be6a",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a \n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to \n",
    "balance the dataset and up-sample the minority class?\n",
    "\n",
    "Ans)\n",
    "\n",
    "Balancing the dataset by up-sampling the minority class can help improve model performance and ensure that the rare event is accurately detected.\n",
    "\n",
    "1. Random Over-Sampling: Random over-sampling involves duplicating instances from the minority class to increase their representation in the dataset. This method can help balance the dataset but may lead to overfitting if not managed carefully\n",
    "\n",
    "2. Synthetic Minority Over-sampling Technique (SMOTE): SMOTE generates synthetic samples for the minority class by interpolating between existing minority class instances. This technique creates new, plausible instances that increase the diversity of the minority class without simply duplicating existing instances\n",
    "\n",
    "3. Adaptive Synthetic Sampling (ADASYN):ADASYN is an extension of SMOTE that focuses on generating more synthetic samples in regions where the minority class is underrepresented and difficult to learn. It adapts the number of synthetic samples generated for different regions based on the density of the minority class instances\n",
    "\n",
    "4. Cluster-Based Over-Sampling: Cluster-based over-sampling involves clustering the minority class instances and then generating synthetic samples within each cluster. This ensures that the synthetic samples are representative of the different subgroups within the minority class.\n",
    "\n",
    "5. Ensemble Methods: Ensemble methods, such as Balanced Random Forest or EasyEnsemble, combine multiple classifiers to improve performance on imbalanced datasets. These methods can incorporate techniques like SMOTE or random under-sampling within the ensemble learning process\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
