{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192dc541-f5bc-4931-81be-82e5fc9d5e46",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its \n",
    "application.\n",
    "\n",
    "ans)\n",
    "\n",
    "Min-Max Scaling also called as Normalization. It is a data preprocessing technique used to transform features of a dataset to a specific range, typically between 0 and 1. It is to standardize the input data before feeding it into algorithms, especially those that are sensitive to the magnitude of data, such as gradient-based optimizers or distance-based algorithms.\n",
    "\n",
    "How it is used in data processing:\n",
    "\n",
    "1. Load Data: Loading data set into the program\n",
    "\n",
    "2. Handle Missing Values: Impute or remove any missing data if necessary.\n",
    "\n",
    "3. Split Data: Divide your dataset into training and testing subsets. It's essential to split the data before scaling to avoid data leakage.\n",
    "\n",
    "4. Initialize Min-Max Scaler: Import and initialize the MinMaxScaler from sklearn.preprocessing.\n",
    "\n",
    "5. Fit and Transform Training Data: Use the fit_transform() method to scale your training data. This calculates the minimum and maximum values from the training set and scales it.\n",
    "\n",
    "6. Transform Test Data: Apply the same scaling (using transform()) to the test set using the previously computed min and max from the training data.\n",
    "\n",
    "7. Train Your Model: Train your machine learning model using the scaled training data.\n",
    "\n",
    "8. Evaluate the Model: Evaluate the performance of your model on the scaled test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da54788-a85e-45c6-a2e8-848fa0dfa9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[22]\n",
      " [25]\n",
      " [28]\n",
      " [30]\n",
      " [35]]\n",
      "\n",
      "Scaled Data:\n",
      " [[0.        ]\n",
      " [0.23076923]\n",
      " [0.46153846]\n",
      " [0.61538462]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Simlpe example in python code for illustrating the Min-Max scalling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data: Ages of individuals\n",
    "ages = np.array([[22], [25], [28], [30], [35]])\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "ages_scaled = scaler.fit_transform(ages)\n",
    "\n",
    "# Print the original and scaled data\n",
    "print(\"Original Data:\\n\", ages)\n",
    "print(\"\\nScaled Data:\\n\", ages_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1182d8e-8d93-444b-ab69-0bffb6fd8b0e",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? \n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "ans)\n",
    "\n",
    "The Unit Vector technique, also known as Normalization, is a feature scaling method that transforms a feature vector into a unit vector. This means that after normalization, the vector will have a length (or Euclidean norm) of 1.\n",
    "\n",
    "Difference from Min-Max Scaling:\n",
    "\n",
    "    1. Min-Max Scaling: Transforms features to a specific range (e.g., [0, 1]) by adjusting the data based on the minimum and maximum values of each feature.\n",
    "\n",
    "    2. Unit Vector Scaling (Normalization): Transforms the feature vector so that its magnitude is 1, focusing on the direction rather than the range of values. This method is independent of the minimum and maximum values and instead normalizes based on the overall magnitude of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2b28d76-3615-4809-a9ed-73fb849ced81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[170  65]\n",
      " [160  70]\n",
      " [180  75]]\n",
      "\n",
      "Normalized Data (Unit Vectors):\n",
      " [[0.93405183 0.35713747]\n",
      " [0.91615733 0.40081883]\n",
      " [0.92307692 0.38461538]]\n"
     ]
    }
   ],
   "source": [
    "#Simlpe example in python code for illustrating the Unit vector\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "# Sample data: height and weight\n",
    "data = np.array([[170, 65],\n",
    "                 [160, 70],\n",
    "                 [180, 75]])\n",
    "\n",
    "# Initialize the Normalizer (unit vector scaling)\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "data_normalized = normalizer.fit_transform(data)\n",
    "\n",
    "# Print the original and normalized data\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nNormalized Data (Unit Vectors):\\n\", data_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443039a-6745-45cc-8aee-5949d08d066f",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an \n",
    "example to illustrate its application.\n",
    "\n",
    "ans)\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a dataset with potentially correlated features into a set of linearly uncorrelated components, called principal components. These components are ordered so that the first few retain most of the variation present in the original dataset.\n",
    "\n",
    "How is it used in dimensionality reduction:\n",
    "\n",
    "    1. Center the Data: Subtract the mean from each feature to center the data around the origin.\n",
    "\n",
    "    2. Compute Covariance Matrix: Calculate the covariance matrix to understand how the features vary with respect to each other.\n",
    "\n",
    "    3. Compute Eigenvalues and Eigenvectors: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions (principal components), and the eigenvalues represent the magnitude (variance) in these directions.\n",
    "\n",
    "    4. Sort Principal Components: Rank the principal components by their eigenvalues, i.e., by the amount of variance they capture.\n",
    "\n",
    "    5. Transform the Data: Project the data onto the top k principal components to reduce the dimensionality while preserving the majority of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f4569b-60d9-4305-8513-ecaea5c76eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[  170    65 45000]\n",
      " [  160    70 48000]\n",
      " [  180    75 52000]\n",
      " [  175    68 50000]]\n",
      "\n",
      "PCA Transformed Data (2 components):\n",
      " [[ 3.74999926e+03 -5.19737633e+00]\n",
      " [ 7.50017169e+02  1.00472452e+01]\n",
      " [-3.25001455e+03 -2.94608428e+00]\n",
      " [-1.25000187e+03 -1.90378454e+00]]\n",
      "\n",
      "Explained Variance Ratio: [9.99994307e-01 5.24344819e-06]\n"
     ]
    }
   ],
   "source": [
    "#Simple illustration of  PCA\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Sample dataset (e.g., height, weight, income)\n",
    "data = np.array([[170, 65, 45000],\n",
    "                 [160, 70, 48000],\n",
    "                 [180, 75, 52000],\n",
    "                 [175, 68, 50000]])\n",
    "\n",
    "# Step 2: Initialize PCA (reduce to 2 components)\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Step 3: Fit and transform the data\n",
    "data_pca = pca.fit_transform(data)\n",
    "\n",
    "# Step 4: Print the transformed data\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nPCA Transformed Data (2 components):\\n\", data_pca)\n",
    "\n",
    "# Step 5: Print the explained variance ratio (how much variance each principal component captures)\n",
    "print(\"\\nExplained Variance Ratio:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ba715-37d5-4879-b6a0-f2da4320b90a",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature \n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "ans)\n",
    "\n",
    "Relationship Between PCA and Feature Extraction:\n",
    "\n",
    "Principal Component Analysis (PCA) is not only a dimensionality reduction technique but also a powerful feature extraction method. In feature extraction, the goal is to derive new features (called components in PCA) from the original features, which better represent the underlying patterns in the data.\n",
    "\n",
    "    Feature Extraction: Refers to transforming the data into a new set of features that may or may not directly correspond to the original features. PCA achieves this by creating a new set of orthogonal features (principal components) that explain most of the variance in the data.\n",
    "\n",
    "    Dimensionality Reduction: Is a side effect of PCA, where we reduce the number of features by selecting only the most significant principal components.\n",
    "    \n",
    "    \n",
    "Relationship Between PCA and Feature Extraction:\n",
    "Principal Component Analysis (PCA) is not only a dimensionality reduction technique but also a powerful feature extraction method. In feature extraction, the goal is to derive new features (called components in PCA) from the original features, which better represent the underlying patterns in the data. These new features capture the most important information while reducing redundancy and noise.\n",
    "\n",
    "    Feature Extraction: Refers to transforming the data into a new set of features that may or may not directly correspond to the original features. PCA achieves this by creating a new set of orthogonal features (principal components) that explain most of the variance in the data.\n",
    "\n",
    "    Dimensionality Reduction: Is a side effect of PCA, where we reduce the number of features by selecting only the most significant principal components.\n",
    "\n",
    "How PCA is Used for Feature Extraction:\n",
    "\n",
    "    1. Capture Maximum Variance: PCA identifies the directions (principal components) in which the data varies the most. These directions are linear combinations of the original features.\n",
    "\n",
    "    2. New Feature Space: Instead of using the original features, PCA transforms the data into a new feature space where each principal component is a new feature. The first few components can capture most of the information in the data, allowing us to discard the less informative components.\n",
    "\n",
    "    3. Uncorrelated Features: PCA generates features (principal components) that are uncorrelated (orthogonal). This can be beneficial when the original features are highly correlated, as the new features provide a more concise and independent representation of the data.\n",
    "\n",
    "    4. Feature Selection: After applying PCA, you can choose the top k principal components as new features. These components often summarize the data better than the original features, especially when there are correlations between features.\n",
    "    \n",
    "Steps to Use PCA for Feature Extraction:\n",
    "\n",
    "    1. Standardize the Data: PCA is sensitive to the scale of data, so standardization is often required.\n",
    "\n",
    "    2. Apply PCA: Perform PCA to extract principal components from the dataset.\n",
    "\n",
    "    3. Select Principal Components: Choose the top k principal components as new features based on how much variance they capture.\n",
    "\n",
    "    4. Use New Features: Replace the original features with the selected principal components for further analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7255368b-c864-4aad-9d9b-f382a4ee309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data (Standardized):\n",
      " [[ 0.         -1.46805055 -1.40693001 -0.32232919]\n",
      " [-1.41421356  0.         -0.16552118  0.48349378]\n",
      " [ 1.41421356  1.46805055  1.4896906  -0.72524067]\n",
      " [ 0.70710678 -0.58722022  0.66208471  1.69222822]\n",
      " [-0.70710678  0.58722022 -0.57932412 -1.12815215]]\n",
      "\n",
      "PCA Transformed Data (2 Principal Components):\n",
      " [[-1.71409327  0.0214244 ]\n",
      " [-0.86086578 -0.06449434]\n",
      " [ 2.56646191 -0.44103472]\n",
      " [ 0.31882658  2.00798075]\n",
      " [-0.31032944 -1.52387609]]\n",
      "\n",
      "Explained Variance Ratio: [0.52319436 0.32766576]\n"
     ]
    }
   ],
   "source": [
    "#Simple illustration example for the concept\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Sample dataset (e.g., height, weight, income, age)\n",
    "data = np.array([[170, 65, 45000, 30],\n",
    "                 [160, 70, 48000, 32],\n",
    "                 [180, 75, 52000, 29],\n",
    "                 [175, 68, 50000, 35],\n",
    "                 [165, 72, 47000, 28]])\n",
    "\n",
    "# Step 2: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Step 3: Apply PCA (extract 2 principal components)\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data_standardized)\n",
    "\n",
    "# Step 4: Print the transformed data\n",
    "print(\"Original Data (Standardized):\\n\", data_standardized)\n",
    "print(\"\\nPCA Transformed Data (2 Principal Components):\\n\", data_pca)\n",
    "\n",
    "# Step 5: Print the explained variance ratio\n",
    "print(\"\\nExplained Variance Ratio:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553d539-b0cb-43d0-8d67-3fb0a3755f71",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset \n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to \n",
    "preprocess the data.\n",
    "\n",
    "ans)\n",
    "\n",
    "Steps to Use Min-Max Scaling:\n",
    "Given a dataset for a food delivery service with features like price, rating, and delivery time, Hence we can apply Min-max scaler:\n",
    "\n",
    "    1. Understand the Features:\n",
    "\n",
    "        Price: The cost of the food item.\n",
    "        Rating: Customer rating of the food item.\n",
    "        Delivery Time: Time taken to deliver the food item.\n",
    "    \n",
    "    2. Standardize Feature Ranges:\n",
    "\n",
    "        Price: Typically ranges from low to high values (e.g., $5 to $100).\n",
    "        Rating: Generally ranges from 1 to 5.\n",
    "        Delivery Time: Can vary widely depending on distance and traffic (e.g., 10 to 60 minutes).\n",
    "        \n",
    "    3. Apply Min-Max Scaling:\n",
    "        Min-Max scaling transforms each feature to a range between 0 and 1 using the formula:\n",
    "        This ensures that all features are on a similar scale, making it easier to compare and use them in a recommendation algorithm.\n",
    "    \n",
    "    4. Preprocess the Data:\n",
    "    \n",
    "        Scale each feature independently using the Min-Max scaling formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c817732-f8f9-475a-a682-c02f6c880b38",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many \n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the \n",
    "dimensionality of the dataset.\n",
    "\n",
    "ans)\n",
    "\n",
    "Steps to Use PCA for Dimensionality Reduction\n",
    "\n",
    "1. Understand the Dataset:\n",
    "\n",
    "    Features: Your dataset might include features such as company financial data (e.g., earnings, revenue, debt levels) and market trends (e.g., interest rates, market indices, trading volumes).\n",
    "    Objective: Reduce the number of features while retaining the most significant information that influences stock prices.\n",
    "\n",
    "2. Preprocess the Data:\n",
    "\n",
    "    Handle Missing Values: Ensure there are no missing values in your dataset or impute them if necessary.\n",
    "    Standardize Features: PCA is sensitive to the scale of the data. Standardizing the features to have zero mean and unit variance is crucial before applying PCA.\n",
    "\n",
    "3. Apply PCA:\n",
    "\n",
    "    Fit PCA: Perform PCA on the standardized dataset to extract principal components.\n",
    "    Select Components: Determine the number of principal components that capture the majority of the variance in the data.\n",
    "\n",
    "4. Transform Data:\n",
    "\n",
    "    Transform: Project the original data onto the selected principal components to obtain a reduced feature set.\n",
    "\n",
    "5. Model Building:\n",
    "\n",
    "    Use Reduced Features: Use the reduced feature set (principal components) for building and training your stock price prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c018ddc3-2be1-44d3-92ac-abdfecf80667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [ 1  5 10 15 20]\n",
      "Scaled Data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the \n",
    "values to a range of -1 to 1.\"\"\"\n",
    "\n",
    "# ans)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Original data\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Calculate min and max\n",
    "X_min = np.min(data)\n",
    "X_max = np.max(data)\n",
    "\n",
    "# Apply Min-Max scaling to range [-1, 1]\n",
    "data_scaled = 2 * (data - X_min) / (X_max - X_min) - 1\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Scaled Data:\", data_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5c39112-1783-47fc-a3e1-7ba1254b9c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio of each component: [9.61182835e-01 3.38382324e-02 3.47182215e-03 1.50711077e-03\n",
      " 4.03340266e-36]\n",
      "Cumulative Variance Ratio: [0.96118283 0.99502107 0.99849289 1.         1.        ]\n",
      "Number of components to keep for 95% variance: 1\n",
      "\n",
      "PCA Transformed Data (Reduced Features):\n",
      " [[-2.74234777]\n",
      " [ 1.45656735]\n",
      " [-0.02328803]\n",
      " [ 3.00305113]\n",
      " [-1.69398269]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform \n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\"\"\"\n",
    "\n",
    "#Ans)\n",
    "#Note: Some additional data points taken for better illustration of results \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Height': [160, 175, 170, 180, 165],\n",
    "    'Weight': [55, 80, 70, 85, 60],\n",
    "    'Age': [25, 40, 35, 50, 30],\n",
    "    'Gender': ['Female', 'Male', 'Male', 'Male', 'Female'],\n",
    "    'Blood Pressure': [120, 140, 130, 150, 125]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocessing\n",
    "# Encode categorical feature 'Gender'\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['Height', 'Weight', 'Age', 'Blood Pressure']),\n",
    "        ('cat', OneHotEncoder(), ['Gender'])\n",
    "    ])\n",
    "\n",
    "# Fit and transform the data\n",
    "df_processed = preprocessor.fit_transform(df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "df_pca = pca.fit_transform(df_processed)\n",
    "\n",
    "# Determine the number of components to retain\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "print(\"Explained Variance Ratio of each component:\", explained_variance)\n",
    "print(\"Cumulative Variance Ratio:\", cumulative_variance)\n",
    "\n",
    "# Choose number of components to keep (e.g., 95% variance)\n",
    "num_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"Number of components to keep for 95% variance: {num_components}\")\n",
    "\n",
    "# Transform data using selected components\n",
    "pca = PCA(n_components=num_components)\n",
    "df_pca_reduced = pca.fit_transform(df_processed)\n",
    "\n",
    "print(\"\\nPCA Transformed Data (Reduced Features):\\n\", df_pca_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9a78f-7c38-40e4-b8e0-c636a772d4cd",
   "metadata": {},
   "source": [
    "Choosing Number of Principal Components reasons:\n",
    "\n",
    "    Variance Threshold: Typically, we would retain enough principal components to capture a significant portion of the total variance (e.g., 95% or 99%). This ensures that you maintain the most critical information in the dataset while reducing dimensionality.\n",
    "\n",
    "    Trade-off: Retaining too few components might lead to loss of important information, while retaining too many might not achieve significant dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9cdfa8-d732-4a55-b6c9-27020e4c3c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
